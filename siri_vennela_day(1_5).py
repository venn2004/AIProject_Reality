# -*- coding: utf-8 -*-
"""SIRI VENNELA DAY(1-5)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XleVQ9IOf6ahqSl69_co0gPVa0HjuqUr
"""

DAY 1

import os
import cv2  # open cv

# Output directory for saving extracted frames
output_folder = "folder"

# Path to the input video file
video_path = "your_video_path"

# Create output folder if it doesn't exist
os.makedirs(output_folder, exist_ok=True)

# Initialize video capture object
cap = cv2.VideoCapture(video_path)

# Validate whether the video opened successfully
if not cap.isOpened():
    print("Error: video is not opening...")
    exit()

frame_count = 0           # Tracks number of frames processed
max_frame = 10            # Number of frames you want to extract

# Loop until the desired number of frames are extracted
while frame_count < max_frame:
    ret, frame = cap.read()     # Read a single frame from the video

    # Break if no frame is returned (end of video or read error)
    if not ret:
        break

    # Construct filename with zero-padded counter (e.g., frame_000.jpg)
    frame_filename = os.path.join(output_folder, f"frame_{frame_count:03d}.jpg")

    # Write the extracted frame to disk
    cv2.imwrite(frame_filename, frame)
    print(f"saved {frame_filename}")

    frame_count += 1     # Increment frame counter

# Release video capture resources and close any OpenCV windows
cap.release()
cv2.destroyAllWindows()
from PIL import Image
from IPython.display import display

folder_path = "your_folder_path"  # folder path in which frames get stored

for file in os.listdir(folder_path):
  file_path = os.path.join(folder_path,file)
  img = Image.open(file_path)
  display(img)
from PIL import Image, ImageFilter
import matplotlib.pyplot as plt

# Opens a image in RGB mode
im = Image.open("photo.jpg")

# Blurring the image
im1 = im.filter(ImageFilter.BoxBlur(40))

# Shows the image in image viewer
plt.imshow(im1)
plt.title("Blurred Image")
plt.axis('off')

import os
from PIL import Image
from IPython.display import display

image_path = "/content/photo-1530092285049-1c42085fd395.jpeg"  # Path to the image file

try:
    img = Image.open(image_path)
    display(img)
    print(f"Successfully displayed image: {image_path}")
except FileNotFoundError:
    print(f"Error: The file '{image_path}' was not found. Please check the path.")
except Exception as e:
    print(f"An error occurred while opening or displaying the image: {e}")

from PIL import Image, ImageFilter
import matplotlib.pyplot as plt

# Opens a image in RGB mode
im = Image.open("/content/photo-1530092285049-1c42085fd395.jpeg")

# Blurring the image
im1 = im.filter(ImageFilter.BoxBlur(40))

# Shows the image in image viewer
plt.imshow(im1)
plt.title("Blurred Image")
plt.axis('off')

"""DAY 2

Audio_Signal_Analysis_and_Spectrogram_Generation_Using_Librosa
"""

import librosa
import numpy as np
import matplotlib.pyplot as plt

# ------------------------------------------------------------
# Load the audio track from the video file
# librosa.load() automatically extracts the audio stream and
# converts it to a time-domain waveform with a target sample rate.
# ------------------------------------------------------------
audio, sr = librosa.load("video_with_audio_path")

# ------------------------------------------------------------
# Compute amplitude envelope of the audio signal by taking
# absolute values of the waveform. This captures loudness patterns.
# ------------------------------------------------------------
amplitude = np.abs(audio)

# ------------------------------------------------------------
# STFT (Short-Time Fourier Transform) to extract frequency content.
# n_fft: size of FFT window
# hop_length: stride between windows
# stft matrix shape: (frequency_bins, time_frames)
# ------------------------------------------------------------
n_fft = 2048
hop_length = 512

stft = np.abs(librosa.stft(audio, n_fft=n_fft, hop_length=hop_length))

# Compute mean amplitude across time for each frequency bin (optional)
freqs = np.mean(stft, axis=1)

# Convert frame indices to actual timestamps in seconds
times = librosa.frames_to_time(
    np.arange(stft.shape[1]), sr=sr, hop_length=hop_length
)

# ------------------------------------------------------------
# Plot amplitude-vs-time. Useful for detecting loud segments,
# silence regions, or envelope patterns.
# ------------------------------------------------------------
plt.figure(figsize=(12, 6))
plt.plot(np.arange(len(audio)) / sr, amplitude)
plt.xlabel("Time (s)")
plt.ylabel("Amplitude")
plt.title("Amplitude over Time")
plt.show()

# ------------------------------------------------------------
# Plot spectrogram: frequency content over time.
# librosa.amplitude_to_db converts raw amplitudes to decibels
# for better interpretability.
# cmap='Accent' is only visual styling.
# ------------------------------------------------------------
plt.figure(figsize=(12, 6))
plt.imshow(
    librosa.amplitude_to_db(stft, ref=np.max),
    cmap="Accent",
    origin="lower",
    aspect="auto"
)
plt.xlabel("Time (s)")
plt.ylabel("Frequency Bin")
plt.title("Frequency over Time (Spectrogram)")
plt.show()

"""Since downloading a sample audio file from external sources has been unreliable, let's generate a simple sine wave audio file directly in Colab."""

import numpy as np
import soundfile as sf
from IPython.display import Audio, display

# ====== SETTINGS ======
duration = 3 # seconds
sample_rate = 16000 # Hz
frequency = 440 # Hz (A4 note)

# ====== GENERATE SINE WAVE ======
t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)
audio_data = 0.5 * np.sin(2 * np.pi * frequency * t)

# ====== SAVE AUDIO FILE ======
output_file = "generated_sample_audio.wav"
sf.write(output_file, audio_data, sample_rate)

print(f"Generated audio file saved as '{output_file}'")
display(Audio(output_file, autoplay=False))

"""Now, we will use the locally generated audio file for the analysis."""

import librosa
import numpy as np
import matplotlib.pyplot as plt

# ------------------------------------------------------------
# Load the audio track from the video file
# librosa.load() automatically extracts the audio stream and
# converts it to a time-domain waveform with a target sample rate.
# ------------------------------------------------------------
audio, sr = librosa.load("generated_sample_audio.wav") # Changed to use the locally generated audio file

# ------------------------------------------------------------
# Compute amplitude envelope of the audio signal by taking
# absolute values of the waveform. This captures loudness patterns.
# ------------------------------------------------------------
amplitude = np.abs(audio)

# ------------------------------------------------------------
# STFT (Short-Time Fourier Transform) to extract frequency content.
# n_fft: size of FFT window
# hop_length: stride between windows
# stft matrix shape: (frequency_bins, time_frames)
# ------------------------------------------------------------
n_fft = 2048
hop_length = 512

stft = np.abs(librosa.stft(audio, n_fft=n_fft, hop_length=hop_length))

# Compute mean amplitude across time for each frequency bin (optional)
freqs = np.mean(stft, axis=1)

# Convert frame indices to actual timestamps in seconds
times = librosa.frames_to_time(
    np.arange(stft.shape[1]), sr=sr, hop_length=hop_length
)

# ------------------------------------------------------------
# Plot amplitude-vs-time. Useful for detecting loud segments,
# silence regions, or envelope patterns.
# ------------------------------------------------------------
plt.figure(figsize=(12, 6))
plt.plot(np.arange(len(audio)) / sr, amplitude)
plt.xlabel("Time (s)")
plt.ylabel("Amplitude")
plt.title("Amplitude over Time")
plt.show()

# ------------------------------------------------------------
# Plot spectrogram: frequency content over time.
# librosa.amplitude_to_db converts raw amplitudes to decibels
# for better interpretability.
# cmap='Accent' is only visual styling.
# ------------------------------------------------------------
plt.figure(figsize=(12, 6))
plt.imshow(
    librosa.amplitude_to_db(stft, ref=np.max),
    cmap="Accent",
    origin="lower",
    aspect="auto"
)
plt.xlabel("Time (s)")
plt.ylabel("Frequency Bin")
plt.title("Frequency over Time (Spectrogram)")
plt.show()

"""First, let's download a sample audio file to work with. This will replace the placeholder `'video_with_audio_path'`."""

!wget -O sample_audio.wav https://www2.cs.uic.edu/~i101/Misc/kalimba.wav

"""Now that we have a sample audio file, let's update the `librosa.load` call in the existing audio analysis cell with the path to this downloaded file.

DAY 3
"""

import pandas as pd

# To load your dataset, you need to either:
# 1. Upload the file directly to Colab (e.g., by dragging and dropping it into the 'Files' tab on the left sidebar).
#    Then, update the path below to '/content/experience-salary-dataset-metadata.csv' (or the correct uploaded name).
# 2. Mount your Google Drive and specify the path to your file.
#    Example: from google.colab import drive
#             drive.mount('/content/drive')
#             data_set = pd.read_csv('/content/drive/My Drive/Your_Folder/experience-salary-dataset-metadata.csv')

# Placeholder path. Please replace with the actual path to your CSV file.
# Using forward slashes for compatibility and to avoid escape sequence warnings.
file_path = '/content/experience-salary-dataset-metadata.csv'

try:
    data_set = pd.read_csv(file_path)
    print(data_set.to_string())
except FileNotFoundError:
    print(f"Error: The file '{file_path}' was not found. Please ensure the file is uploaded or the path is correct.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

data_set= pd.read_csv('/content/Experience-Salary.csv')
print(data_set.head()) # Correctly calls the head() method
# To view other parts of the DataFrame, you can use:
# print(data_set.tail())
# data_set.info()
# print(data_set.describe())

x= data_set.iloc[:, :- 1].values
y= data_set.iloc[:, 1].values
print(x)
print(y)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 1/3, random_state=0)
print(x_test,y_test)

# -----------------------------
# 1. Import Libraries
# -----------------------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# -----------------------------
# 2. Load the main dataset
# -----------------------------
# IMPORTANT: Load the actual CSV, not metadata
data_set = pd.read_csv('/content/Experience-Salary.csv')
print(data_set.head())

# -----------------------------
# 3. Split into X (Experience) and y (Salary)
# -----------------------------
X = data_set.iloc[:, :-1].values      # Experience
y = data_set.iloc[:, 1].values        # Salary

print("X:", X.flatten())              # Flatten for better printing
print("y:", y)

# -----------------------------
# 4. Trainâ€“Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=1/3, random_state=0
)

# -----------------------------
# 5. Train Linear Regression Model
# -----------------------------
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# -----------------------------
# 6. Predict on Test Data
# -----------------------------
y_pred = regressor.predict(X_test)

print("Predicted:", y_pred)
print("Actual:", y_test)

# -----------------------------
# 7. Plot Salary vs Experience (Training Set)
# -----------------------------
plt.figure(figsize=(8,5))
plt.scatter(X_train, y_train)
plt.plot(X_train, regressor.predict(X_train))
plt.title("Salary vs Experience (Training Data)")
plt.xlabel("Experience (Years)")
plt.ylabel("Salary")
plt.show()

# -----------------------------
# 8. Plot Salary vs Experience (Test Set)
# -----------------------------
plt.figure(figsize=(8,5))
plt.scatter(X_test, y_test)
plt.plot(X_train, regressor.predict(X_train))
plt.title("Salary vs Experience (Test Data)")
plt.xlabel("Experience (Years)")
plt.ylabel("Salary")
plt.show()

plt.scatter(x_test, y_test, color="blue")
plt.plot(x_train, regressor.predict(x_train), color="red") # Changed x_pred to regressor.predict(x_train) for consistency
plt.title("Salary vs Experience (Test Dataset)")
plt.xlabel("Years of Experience")
plt.ylabel("Salary(In Rupees)")
plt.show()

"""DAY 4

TEXT TO SPEECH
"""

!pip install numpy soundfile


import numpy as np
import soundfile as sf
from IPython.display import Audio, display

# ====== SETTINGS
duration = 3
sample_rate = 16000 # 16 kHz audio
frequency = 440

# ====== GENERATE SINE WAVE
t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)
audio_data = 0.5 * np.sin(2 * np.pi * frequency * t)

# ====== SAVE AUDIO FILE ======
output_file = "generated_audio.wav"
sf.write(output_file, audio_data, sample_rate)

# ====== PRINT OUTPUT =
print("Audio file created successfully!")
print("Saved as:", output_file)

# ====== PLAY AUDIO IN COLAB ======
display(Audio(output_file, autoplay=True))

import soundfile as sf
from IPython.display import Audio, display
from mutagen.wave import WAVE
from mutagen.id3 import TextFrame

# ====== SETTINGS ======
duration = 3
sample_rate = 16000 # 16 kHz audio
# Sine tone frequency
frequency = 440

# ====== GENERATE SINE WAVE ======
t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)
audio_data = 0.5 * np.sin(2 * np.pi * frequency * t)

# ====== SAVE AUDIO FILE ======
output_file= "generated_audio_with_info.wav"
sf.write(output_file, audio_data, sample_rate)

# ====== ADD METADATA ======
metadata = WAVE(output_file)

# Use TextFrame for metadata values
metadata["INAM"] = TextFrame(encoding=3, text=["Sample Tone Audio"])
metadata["IART"] = TextFrame(encoding=3, text=["Your Name"])
metadata["ICMT"] = TextFrame(encoding=3, text=["This is a generated audio tone with embedded metadata"])
metadata["ICRD"] = TextFrame(encoding=3, text=["2025-02-14"])
metadata.save()

# ====== PLAY AUDIO ======
print("Audio file created with metadata!\n")
display(Audio(output_file, autoplay=False))
# -===. READ AND PRINT METADATA ......
print(" === Embedded Metadata in WAV File === ")
for key, value in metadata.items():
    print(f"{key}: {value.text[0] if hasattr(value, 'text') else value}")

"""TEXT TO SPEACH"""

!pip install gTTS soundfile # google text to speech

from IPython.display import Audio, display
import soundfile as sf
import numpy as np
from gtts import gTTS # Import gTTS here

# ====== TEXT TO SPEECH INPUT ======
text = "Hello! This is an automatically generated audio message created in Google Colab."

# ====== GENERATE AUDIO FROM TEXT ======
tts = gTTS(text=text, lang='en') # Fixed the unterminated string literal
tts.save("text_audio.mp3")

# Convert MP3 to WAV (optional)
# Load MP3 using audio libraries
import librosa
audio_data, sr = librosa.load("text_audio.mp3", sr=16000)
sf.write("text_audio.wav", audio_data, sr)

# ====== PLAY AUDIO ======
print("Text converted to speech and saved as 'text_audio.wav'")
display(Audio("text_audio.wav", autoplay=False))

"""DAY 5
VOSK WHISPER
"""

!pip install -q vosk gTTS faster-whisper pydub soundfile sentencepiece
!apt -qq install -y ffmpeg

from google.colab import files
from gtts import gTTS
from IPython.display import Audio
import os, wave, json, subprocess

# --.. AUDIO PREP --.-
print("Upload audio (optional). Skip to auto-create TTS sample.")
up = files.upload()

if up:
    audio = list(up.keys())[0]
else:
    tts = gTTS("Hello! This is a test audio for comparing Vosk and Whisper.", lang="en")
    audio = "sample.mp3"
    tts.save(audio)

# Convert + WAV 16khz mono
wav = "audio.wav"
subprocess.run(["ffmpeg","-y","-i",audio,"-ar","16000","-ac","1",wav],
               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

display(Audio(wav))
# ---- VOSK ----
if not os.path.exists("vosk-model"):
    !wget -q https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip -O model.zip
    !unzip -q model.zip
    !mv vosk-model-small-en-us-0.15 vosk-model
    !rm model.zip

from vosk import Model, KaldiRecognizer
wf = wave.open(wav, "rb")
rec=KaldiRecognizer(Model("vosk-model"),wf.getframerate())
vosk_text=""
while True:
    data = wf.readframes(4000)
    if not data: break
    if rec.AcceptWaveform(data):
        vosk_text += json.loads(rec.Result()).get("text"," ") +""
vosk_text += json.loads(rec.FinalResult()).get("text"," ")

print("\n>>> VOSK:\n", vosk_text.strip())

# ---- WHISPER
import torch
from faster_whisper import WhisperModel

device = "cuda" if torch.cuda.is_available() else "cpu"
wmodel = WhisperModel("small", device=device)

segments, _ = wmodel.transcribe(wav)
whisper_text = " ".join([s.text for s in segments]).strip()

print("\n>>> WHISPER:\n", whisper_text)



"""DAY 6

"""

